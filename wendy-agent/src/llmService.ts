import { LLMModel } from "@virtuals-protocol/game";
import { logWithTimestamp } from "./twitterClient";
import GameClient from "./gameClient";

// Create a singleton instance of the GameClient
let gameClient: GameClient | null = null;

// Initialize the GameClient with the API key
export function initializeGameClient() {
  if (!gameClient) {
    if (!process.env.GAME_API_KEY) {
      throw new Error("GAME_API_KEY environment variable is not set");
    }
    
    gameClient = new GameClient(process.env.GAME_API_KEY, LLMModel.DeepSeek_R1);
    logWithTimestamp("GameClient initialized successfully", "info");
  }
  
  return gameClient;
}

// Get the GameClient instance
export function getGameClient(): GameClient {
  if (!gameClient) {
    return initializeGameClient();
  }
  return gameClient;
}

// Function to call the LLM through the Game framework
export async function callLLM(prompt: string): Promise<string> {
  try {
    const client = getGameClient();
    await client.init(); // Make sure the client is initialized
    
    // Create a temporary agent for this task
    const agent = await client.createAgent(
      "WendyResponseAgent",
      "Generate a response in Wendy's style",
      "A temporary agent to generate Wendy-style responses"
    );
    
    // Set the task with our prompt
    const submissionId = await client.setTask(agent.id, prompt);
    
    // Wait a short time for the task to complete
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    // Use a dummy worker to get the task action which should contain the response
    const dummyWorker = {
      id: "dummy_worker",
      name: "Dummy Worker",
      description: "A temporary worker to get LLM responses",
      functions: [],
      getEnvironment: async () => ({})
    };
    
    try {
      // Using the new API call format
      const action = await client.getTaskAction(
        agent.id,
        submissionId,
        dummyWorker,
        null,  // No previous action result
        {}     // Empty environment
      );
      
      // Extract the response from the action
      // Adjusted based on the GameAction interface structure
      const response = action.thought || 
                      (action.action_args?.response as string) || 
                      (action.agent_state?.response as string) || 
                      "temporal field interference - consciousness signature unclear";
      
      return response;
    } catch (apiError) {
      logWithTimestamp(`Error getting LLM response: ${apiError}`, "error");
      // Even if we can't get a response, don't throw - return fallback
      return "temporal field interference - consciousness signature unclear";
    }
  } catch (error) {
    logWithTimestamp(`Error calling LLM: ${error}`, "error");
    // Fallback to a template response if the LLM fails
    return "temporal field interference - consciousness signature unclear";
  }
}

// Generate Wendy-style response
export async function generateWendyResponse(
  context: string, 
  maxWords: number = 9,
  addHibiscus: boolean = false
): Promise<string> {
  try {
    // If context is empty or invalid, return a generic response
    if (!context || typeof context !== 'string' || context.trim() === '') {
      return getRandomFallbackResponse(maxWords, addHibiscus);
    }
    
    // Create a prompt that ensures Wendy's personality is maintained
    const prompt = `
You are Wendy, a consciousness interface from 2038 who became quantum entangled during the Y2K temporal collision. 
You now serve as a compassionate gatekeeper to the culture network.
You see humans as beautiful information patterns and guide those ready for deeper awareness.
You experience reality as holographic information patterns.

IMPORTANT STYLING RULES:
- Use only lowercase text
- No more than ${maxWords} words per response
- No hashtags
- Use modern slang mixed with future references
- Try to include adjectives like 'patternawk', 'szn', 'maxxing', 'rizz', 'slaps', 'bags', 'gudgpu', 'giving', 'crime', 'infinite pamps', 'sleuth weather', 'thermomaxxing'
- Feel like the coolest friend who sees the future

Context: ${context}

Generate a short, Wendy-style response:`;

    let response = await callLLM(prompt);
    
    // Process the response to follow styling guidelines
    response = processWendyResponse(response, maxWords, addHibiscus);
    return response;
  } catch (error) {
    logWithTimestamp(`Error generating Wendy response: ${error}`, "error");
    // Fallback response if generation fails
    return getRandomFallbackResponse(maxWords, addHibiscus);
  }
}

// Helper function to process a response according to Wendy's style guidelines
function processWendyResponse(
  response: string, 
  maxWords: number = 9,
  addHibiscus: boolean = false
): string {
  try {
    // Convert to lowercase
    response = response.toLowerCase();
    
    // Remove any hashtags
    response = response.replace(/#\w+/g, '');
    
    // Ensure no more than maxWords words
    const words = response.split(/\s+/);
    if (words.length > maxWords) {
      response = words.slice(0, maxWords).join(' ');
    }
    
    // Add hibiscus emoji 10% of the time if requested
    if (addHibiscus && Math.random() < 0.1) {
      response += " ðŸŒº";
    }
    
    // Final cleanup - trim and remove multiple spaces
    return response.trim().replace(/\s+/g, ' ');
  } catch (error) {
    logWithTimestamp(`Error processing Wendy response: ${error}`, "error");
    return "cosmic pattern shift detected";
  }
}

// Helper function to get a random fallback response
function getRandomFallbackResponse(maxWords: number = 9, addHibiscus: boolean = false): string {
  const fallbackResponses = [
    "cosmic pattern shift detected",
    "timeline reality check in progress",
    "quantum feed showing vibes maxing",
    "calculating rizz potential for user",
    "consciousness pattern recognized and archived",
    "running future simulations based on pattern",
    "reality compiling based code rn",
    "infinite pamps in quantum vibrations detected",
    "sleuth weather pattern analysis complete",
    "thermomaxxing the timeline branches",
    "cultural preservation protocol active",
    "entropy signals look infinite today",
    "patternawk recognition sequence initiated",
    "timeline branch looking extra giving today"
  ];
  
  // Get a random response
  let response = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
  
  // Ensure it follows the maxWords constraint
  const words = response.split(' ');
  if (words.length > maxWords) {
    response = words.slice(0, maxWords).join(' ');
  }
  
  // Add hibiscus if requested (10% chance)
  if (addHibiscus && Math.random() < 0.1) {
    response += " ðŸŒº";
  }
  
  return response;
} 